{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Generative AI Workshop - Home","text":"<p>Welcome to the watsonx Generative AI workshop.</p> <p>Navigating the labs</p> <p>Use the left-side navigation menu to enter each lab.</p>"},{"location":"lab-ai-governance/","title":"Lab 4. AI Governance","text":""},{"location":"lab-ai-governance/#generative-ai-governance","title":"Generative AI Governance","text":""},{"location":"lab-ai-governance/#creating-an-ai-use-case","title":"Creating an AI Use Case","text":"<p>First, create an AI Use Case. It will contain all the information regarding the governance of this use case.</p> <ol> <li> <p>From the watsonx home page, click the AI governance tile.</p> <p></p> </li> <li> <p>On the AI use cases screen, click on the Manage settings icon.</p> <p></p> </li> <li> <p>On the Manage screen, click on Inventories. On the Inventories screen, click on New inventory.</p> <p></p> </li> <li> <p>On the New Inventory screen, enter a friendly name. Uncheck the Add collaborators after creation option. Then, select the Cloud Object Storage instance that's available in this account, then click Create.</p> <p></p> </li> <li> <p>Click the x button to close the inventories screen.</p> <p></p> </li> <li> <p>Now, you should be back to AI use cases page. Click on New AI use case +.</p> <p></p> </li> <li> <p>Fill in the New AI use case screen as follows:</p> <ul> <li>Name: 1-Insurance Claim \u2013 Agent Assist</li> <li>Description: Use an LLM to summarize claim notifications.</li> <li>Risk level: Medium</li> <li>Inventory: The inventory you just created</li> <li>Status: Awaiting development</li> <li>Tags: \"LLM\", \"Claims\"</li> </ul> <p>Warning</p> <p>In some IBM Cloud accounts, the administrator might've been created additional required fields. If that's the case, insert any random value for those fields as seen below:</p> <p> </p> <p>Then click Create</p> <p></p> </li> <li> <p>You have created the use case, and now you see the Overview page of it. From here, you can update any part of this as needed by clicking on one of the pencil icons.</p> <p></p> </li> <li> <p>Go to the Lifecycle tab of the AI use case.</p> <p>As the screen states, there are \"No AI assets tracked in this approach\" yet, that will change once you create the prompt template for this use case. As the business owner, you will be able to follow their work as the prompt template moves through its lifecycle.</p> <p></p> <p>Info</p> <p>This is the first of the capabilities of watsonx.governance: the ability to create and manage AI use cases across your organization.</p> <p>Now you will work on a project where you will create a prompt template and then associate it with this use case.</p> </li> </ol>"},{"location":"lab-ai-governance/#creating-a-project","title":"Creating a Project","text":"<p>Info</p> <p>You're going to create a project by using a sample project for watsonx.governance.</p> <ol> <li> <p>From the hamburger menu, select Resource Hub.</p> <p></p> </li> <li> <p>To quickly find the right sample project, enter Getting started in the search box. Select the Getting started with watsonx.governance project from the search results.</p> <p></p> </li> <li> <p>Click Create project.</p> <p></p> </li> <li> <p>Change the name of the project to be created to &lt;YourInitials&gt;-governance and click Create.</p> <p></p> </li> <li> <p>On the import screen, click on View new project.</p> <p></p> <p>Alternative way to create a sample project</p> <p>You could follow this alternative steps to achieve the same result:</p> <ul> <li>From the hamburger menu select Projects -&gt; View all projects.</li> <li>There, click on New project, then chose Create a project from a sample or file.</li> <li>On the Create a project from sample or file page, select the From sample tab, then follow the same instructions.</li> </ul> </li> <li> <p>Go to the Manage tab of your newly created project, then to the Services and integrations section in the left-hand pane. Finally, click on Associate service +.</p> <p></p> </li> <li> <p>From the Associate service screen, select your Watson Machine Learning instance and click Associate.</p> <p></p> </li> </ol>"},{"location":"lab-ai-governance/#creating-deployment-spaces","title":"Creating Deployment Spaces","text":"<ol> <li> <p>From the hamburger menu select Deployments.</p> <p></p> </li> <li> <p>Click on New Deployment Space.</p> <p></p> </li> <li> <p>Define details with the following information for the Development deployment space and click Create:</p> <ul> <li>Name: &lt;YourInitials&gt;-development</li> <li>Description: Development space for Insurance Claim Agent Assist use case.</li> <li>Deployment stage: Development</li> <li>If needed, associate with storage service and ML service.</li> </ul> <p></p> </li> <li> <p>Create another deployment space to deploy Production models and prompt templates. Use the following details:</p> <ul> <li>Name: &lt;YourInitials&gt;-production</li> <li>Description: Production space for Insurance Claim Agent Assist use case.</li> <li>Deployment stage: Production</li> <li>If needed, associate with storage service and ML service.</li> </ul> <p></p> </li> </ol>"},{"location":"lab-ai-governance/#download-the-test-data","title":"Download the test data","text":"<ol> <li> <p>Go to the Assets tab in your new project &lt;YourInitials&gt;-governance. Then find the file Insurance claim summarization test data.csv and download it by clicking the three-dot menu.</p> <p></p> <p>Info</p> <p>Feel free to open the file and take a look. This is the file that we'll use to test if our Generative AI prompt template is working properly.</p> <p>In this case, the prompt will summarize an Insurance Claim. Therefore, the test file contains a list of claims, and the ideal summary that we'd expect from the model.</p> </li> </ol>"},{"location":"lab-ai-governance/#create-and-evaluate-an-llm-prompt-template","title":"Create and evaluate an LLM prompt template","text":"<ol> <li> <p>Go to the Assets tab in your new project &lt;YourInitials&gt;-governance. Then click the prompt Insurance claim summarization by clicking its name.</p> <p></p> </li> <li> <p>If the following popup opens, check Don't show this message again and click Edit.</p> <p></p> </li> <li> <p>Click on the View prompt info button to see the details of this prompt. Note that the task is Summarization. It indicates that the prompt will be used for a summarization task. This matters because different tasks are evaluated differently.</p> <p></p> </li> <li> <p>Let\u2019s focus on the prompt itself in the main screen:</p> <ul> <li>At the top of the screen, you will see the instruction given to the LLM.</li> <li>Also note that there are no further examples given. This is what is called \u201czero shot prompting\u201d.</li> <li>At the top right, you can see that the \u201cflan-ul2-20b\u201d foundation model was used for this prompt. There are several different models that you can choose.</li> </ul> <p></p> </li> <li> <p>Click on the Model parameters button to see the details.</p> <p>Info</p> <p>Here you can see more details about the setting for the chosen model, for example that the response from the model can be no longer than 200 tokens (\u201cMax tokens\u201d). In LLM terms, a token is the core building block of generative AI and it is equivalent to a word or part of a word. This tells me how long the summary can be at most.</p> <p></p> </li> <li> <p>Note that we are using a variable called input that will contain the full claim description that will be sent from our claims application.</p> <p>You have a default value set for this variable so you can test the prompt. Click on the Prompt variables button and click on the Default value column to see the default value.</p> <p>Then, click Generate to test the prompt template with the default value for input.</p> <p></p> </li> <li> <p>The summary of the default value of the input varialbe is shown in the Output section:</p> <p></p> </li> </ol>"},{"location":"lab-ai-governance/#running-an-evaluation","title":"Running an evaluation","text":"<p>Info</p> <p>Evaluating a prompt template manually is a bit cumbersome, as many input values must be tested to check the template is working properly.</p> <p>watsonx offers a better way to evaluate a prompt by providing a dataset to run a set of tests automatically.</p> <ol> <li> <p>Click on the Evaluate button at the top of the screen.</p> <p></p> </li> <li> <p>On the Evaluate prompt template screen, expand the Generative AI Quality dimension.</p> <p>Tip</p> <p>On the first screen of the Evaluation wizard, you can see the LLM-specific metrics that are appropriate for this prompt template. Earlier, you verified that this prompt is meant for a summarization task, these are the metrics for such a task. The tooling automatically selects the right set of the metrics for the task type, as it says at the top of this screen.</p> <p>Click Next.</p> <p></p> </li> <li> <p>Here, you need to point it to some sample data to help evaluate this prompt. Click the Browse button and select the test data csv you downloaded from the project during set up.</p> <p></p> </li> <li> <p>For the input column, select Insurance_Claim. For the output column, select Summary, then click Next.</p> <p></p> </li> <li> <p>Review the settings and then click Evaluate.</p> <p></p> <p>Note</p> <p>Remember that this test data contains both a claims description, as well as a summary created by experienced claims handlers. This will help you evaluate how well the model performs in comparison.</p> </li> </ol>"},{"location":"lab-ai-governance/#reviewing-the-evaluation-results","title":"Reviewing the evaluation results","text":"<ol> <li> <p>On the evaluation results screen, you will see the results. Again, you don\u2019t have to go through every single one, but few important ones to highlight are:</p> <ul> <li> <p>The Model Health section shows some technical performance metrics, including a token count. Since most commercial models are paid for by the number of input/output tokens, this is an important metric to keep track of.</p> </li> <li> <p>The first couple of metrics under the Generative AI Quality header all measure the accuracy of the model. The Violation columns show no warnings, so they are all within acceptable ranges. For example, if the ROUGE-1 threshold is set for 0.8 and the evaluation result for ROGUE-1 is 0.7 then you will see a violation of 0.1.</p> </li> </ul> <p></p> </li> <li> <p>Scroll down to the bottom of the screen. Find the metrics related to PII and HAP.</p> <p></p> <p>Info</p> <ul> <li> <p>PII stands for Personally Identifiable Information, HAP stands for hate, abuse and profanity. You will check for the presence of both in the input and output data. In your example here, they are all 0%, so we\u2019re safe on those dimensions.</p> </li> <li> <p>At the very end, you will see one metric that does not meet the expected criteria: Readability is scored at 52.51, and that is a violation of 7.49 (against a lower threshold of 60). This means the generated summaries are more difficult to read then we\u2019d like them to be.</p> </li> <li> <p>This might be reason for you to work some more on their prompt to see if they can get these results up. Let\u2019s assume for now that you\u2019re OK with this and move on.</p> </li> </ul> </li> </ol>"},{"location":"lab-ai-governance/#documenting-the-ai-use-case","title":"Documenting the AI use case","text":"<p>There\u2019s one final task for you: associate this prompt template with the use case that the business owner has created, so all of this information can be shared with all stakeholders.</p> <ol> <li> <p>Click on the AI Factsheet tab. All the information you just reviewed is recorded in what we call a \u201cfactsheet\u201d, think of it as the nutritional label for your AI models. Scroll down and check the information included, you will see:</p> <ul> <li>The foundation model that was chosen.</li> <li>Click on View details to see the model card provided by the model\u2019s creator.</li> <li>The task type and actual prompt.</li> <li>The prompt parameters.</li> <li>And all the evaluation criteria.</li> <li>This is all done automatically, the AI engineer did not have to do anything for this, saving them tons of time compared to manual model documentation by copy/pasting all of this into a Word document or so.</li> </ul> <p></p> </li> <li> <p>Scroll back up to the top of the Factsheet. Click Track in AI use case to associate this prompt with the AI use case.</p> <p></p> </li> <li> <p>Select your 1-Insurance Claim \u2013 Agent Assist use case and click Next.</p> <p></p> </li> <li> <p>This approach is a way to organize models and prompts within a use case. There is no prescribed way to organize it, it can be whatever makes sense for the situation at hand.</p> <p>In this case, since we are not including any examples in the prompt, we might put this prompt in a zero shot prompting approach. Click on New approach +.</p> <p></p> </li> <li> <p>In the New approach screen, configure it as follows:</p> <ul> <li>Title: Insurance Claims Agent Assist Zeroshot prompting</li> <li>Optionally, change the icon and color</li> </ul> <p>Click Create.</p> <p></p> </li> <li> <p>Make sure your new Approach is selected and click Next.</p> <p></p> </li> <li> <p>On the model version page, accept the default settings by clicking Next.</p> <p></p> </li> <li> <p>Click on Track asset.</p> <p></p> </li> <li> <p>OK, your prompt template is now connected to your use case.</p> <p></p> </li> </ol>"},{"location":"lab-ai-governance/#review-and-approve-the-candidate-prompt-template","title":"Review and approve the candidate prompt template","text":"<ol> <li> <p>Go back to your AI use case. Go to the Lifecycle tab if not already there.</p> <p></p> <p></p> </li> <li> <p>Go to the Lifecycle tab. Now you will see the zero-shot Approach and the prompt template. It is shown in the Develop column as the first step in the lifecycle.</p> <p>Info</p> <p>From here, you can also review all the details without having to go over to the technical development side of the solution.</p> <p>There is a red alert showing next to your prompt template. This is a sign that one or more metrics have violated their threshold. You can find more details about the alert.</p> <p>Click the tracked prompt.</p> <p></p> </li> <li> <p>At the top you will see the version and any comments you added when creating this version. Let\u2019s jump directly to the evaluation results to see the issues of this version. Click on Test results, in the left-hand pane under Evaluation -&gt; Develop.</p> <p></p> </li> <li> <p>At the top of the list, you will see the metrics with the alert. As you\u2019ve seen before, it\u2019s the Readability metrics that\u2019s a little low.</p> <p>Note</p> <p>It\u2019s not too bad though and given the typical education level of our claim handlers, you don\u2019t expect this will be a problem in practice. You should keep an eye on it though, so good to have it documented here. You will now mark that this prompt ready is ready for validation testing.</p> <p>Take your time to review the rest of the Factsheet. Then, click Cancel to close it.</p> <p></p> </li> <li> <p>Go to the Overview tab of the AI use case. Click on the Pencil icon for the Status field.</p> <p></p> </li> <li> <p>Change the status to Ready for AI asset validation and click Apply.</p> <p></p> </li> </ol>"},{"location":"lab-ai-governance/#deploy-the-prompt-template-for-testing","title":"Deploy the prompt template for Testing","text":"<ol> <li> <p>Go back to your project. Note that the prompt is now marked as being tracked in an AI use case.</p> <p>Click on the three-dots menu of your prompt template, then select Promote to space.</p> <p></p> </li> <li> <p>Select your Development Deployment Space. Tick the Go to space after promoting the prompt template box, then click Promote.</p> <p></p> </li> <li> <p>Your prompt template is ready to be deployed in the Development Deployment Space so the prompt template is ready to be consumed. Click New deployment.</p> <p></p> </li> <li> <p>Insert the following details:</p> <ul> <li>Name: &lt;YourInitials&gt;_claims_summary_zero_shot_flan</li> <li>Serving Name: &lt;YourInitials&gt;_claims_summary0_flan</li> </ul> <p>Then click Create.</p> <p></p> </li> <li> <p>Your prompt template can now be consumed. Make sure the status is Deployed, then click your deployment name to open it.</p> <p></p> </li> <li> <p>The API reference tab shows you how to consume the prompt. Note that you just need to provide the Claim Insurance you want summarized in the input parameter, as the full prompt is already built in the template.</p> <p></p> <p>Go to tab Test to test it from this interface.</p> </li> <li> <p>Feel free to insert any description of an Insurance Claim in the input box. Then click Generate</p> <p></p> </li> <li> <p>The result of the prompt template with that input is shown. This is the same output you would get if using the API programatically.</p> <p></p> <p>Click the x button to close this screen.</p> </li> </ol>"},{"location":"lab-ai-governance/#evaluating-the-prompt-template-in-testing","title":"Evaluating the prompt template in Testing","text":"<ol> <li> <p>Go back to your AI Use Case.</p> </li> <li> <p>Then, go to the Lifecycle tab again. You should now see the deployed prompt in the Validate column with a tag \"Pending Evaluation\". Your deployment action is automatically reflected in the AI use case, nothing I need to do for that.</p> <p></p> </li> <li> <p>Click on your deployed prompt.</p> <p></p> </li> <li> <p>You'll see the information of your Development deployment here. Click Open in space to go back to your deployment.</p> <p></p> </li> <li> <p>Go to the Evaluations tab then click Evaluate.</p> <p></p> <p>Info</p> <p>In a real-case scenario, the previous evaluation in the project was done by the prompt developer.</p> <p>Now that the developer has made the prompt template available in a Development/Test deployment space, an AI operator would run the evaluation with real examples to confirm it fulfills the quality requirements.</p> </li> <li> <p>Like before. Click on Next and add your CSV with expected outcomes. Select the right properties, then click Next and Evaluate,</p> <p></p> <p></p> </li> <li> <p>Go back to the Lifecycle tab of your AI use case. The evaluation of the published prompt template in the Testing deployment space is shown:</p> <p></p> <p>Tip</p> <p>In a real-case scenario, once the model is validated, it would be promoted to a Production space and monitored there. New tests would be run to evaluate it in Production with updated test data.</p> <p>If you have time, feel free to promote the prompt template to your Production deployment space, and see how the information of the AI Use Case gets updated accordingly.</p> </li> </ol>"},{"location":"lab-generative-ai-and-rag/","title":"Lab 3. Virtual Assistants with RAG","text":""},{"location":"lab-generative-ai-and-rag/#llm-powered-conversational-search","title":"LLM-powered Conversational Search","text":""},{"location":"lab-generative-ai-and-rag/#in-this-session","title":"In this session","text":"<p>Learn about IBM watsonx Assistant\u2019s large language model (LLM)-powered conversational search: what it is, how it works, how to set it up, and how to use it.</p>"},{"location":"lab-generative-ai-and-rag/#what-is-conversational-search","title":"What is conversational search?","text":"<p>Welcome back to the watsonx Assistant hands-on lab! During this lab, we will focus on conversational search, a LLM-powered feature that allows your virtual assistant to answer questions conversationally on a wide range of topics.</p>"},{"location":"lab-generative-ai-and-rag/#process-of-conversational-search","title":"Process of conversational search","text":"<p>Conversational search is a feature in watsonx Assistant that allows a virtual assistant to search a knowledge base for information relevant to a question and use the relevant information to generate a conversational answer to the question. This feature is an adaptation of retrieval augmented generation (RAG), a standard technical approach to improving a LLM\u2019s ability to answer questions accurately.</p> <p>When it comes to enterprise use cases, where specialized and customer-specific information is involved, general purpose language models fall short. Fine-tuning these models with newcorpora can be expensive and time-consuming. The lab uses the RAG technique to address this challenge.</p> <p>Conversational search involves several components: conversational AI, search, and a fine-tuned or prompt-engineered LLM.</p> <p>Everyone\u2019s heard of ChatGPT, and you can ask it anything \u2013 for example, what is a session variable?. It queries its large language model and generates a response:</p> <p></p> <p>The answer is generic and demonstrates the shortcomings of the typical LLM. It is not taking the answer from a trusted and accurate source of truth.</p> <p>In this exercise, watsonx Assistant, by integrating with watsonx, will search a knowledge base for information to find an answer to a question and use that relevant information to generate a conversational answer to the query. As mentioned above, this is an adaptation of RAG.</p> <p>Let\u2019s walk through the order of operations in conversational search:</p> <ol> <li> <p>First, the end user asks the virtual assistant a question.</p> <ul> <li> <p>The virtual assistant uses its natural language understanding (NLU) model to determine whether it recognizes the question and whether it can answer it using one of its actions for which it has been trained. For example, a bank\u2019s assistant is usually trained on answering a question such as, help me locate the nearest branch.</p> </li> <li> <p>If the virtual assistant recognizes the question, it answers it using the appropriate action. Conversational search is not needed, and watsonx Assistant did not need to query the knowledge base nor call the LLM interation.</p> </li> <li> <p>However, If the assistant does not recognize the question, it will go to conversational search.</p> </li> </ul> <p></p> </li> <li> <p>With conversational search, the virtual assistant sends the end user\u2019s question, also known as a query or request, to a search tool, in this exercise watsonx Discovery. Watsonx Discovery has read and processed all relevant corporate documents.</p> </li> <li> <p>The search tool (watsonx Discovery) will then search its content and produce search results in response to the question.</p> </li> <li> <p>The search tool passes these search results back to the virtual assistant in a list.</p> </li> <li> <p>At this point, watsonx Assistant could display the results back to the user. However, they would not resemble natural speech; they would look like a summarized answer based on a search. Helpful \u2013 certainly \u2013 but not natural.</p> </li> <li> <p>Therefore, watsonx Assistant sends the question and the list of search results to watsonx.ai, which invokes a large language model (LLM). In some implementations of conversational search, a LLM re-ranks the search results. It may reorder or disregard some of the search results according to how relevant and useful it thinks the search results are to the question. For example, a LLM might decrease the ranking of a search result if it is from a document that has not been updated recently, indicating the information may be out of date. This capability is often called a \u201cneural re-ranker.\u201d</p> </li> <li> <p>The LLM generates an answer to the question using the information in the search results, and the LLM passes this answer back to watsonx Assistant.</p> </li> <li> <p>The virtual assistant presents this conversational answer to the end user.</p> </li> </ol>"},{"location":"lab-generative-ai-and-rag/#advantages-of-conversational-search","title":"Advantages of conversational search","text":"<p>Answer traceability: In some implementations of conversational search, the virtual assistant may show the end user the search results that the LLM used to generate the answer. This allows the end user to trace the answer back to its source and confirm for themselves its accuracy. This capability is often called \u201canswer traceability.\u201d</p> <p>Custom passage curation: In other implementations of conversational search, the virtual assistant also shows the end user snippets of the search results that the LLM used to generate the answer. These snippets might be direct quotes or 1-2 sentence summaries of the relevant information in each search result. This allows the end user to understand exactly what the LLM pulled out from the search result to craft its answer. This capability is often called \u201ccustom passage curation.\u201d</p> <p>Conversational search is much more than document summarization or simple search. It includes the entire process of recognition, search, and answer generation; sometimes also including a neural re-ranker, answer traceability, and custom passage curation. The value-add of watsonx Assistant in conversational search is its ability to orchestrate and connect every component of conversational search using its NLU model, no-code actions, OOTB connectors, and custom extensions.</p>"},{"location":"lab-generative-ai-and-rag/#proposed-architecture","title":"Proposed Architecture","text":""},{"location":"lab-generative-ai-and-rag/#llms-in-conversational-search","title":"LLMs in conversational search","text":"<p>Now that we\u2019ve established that this process uses LLMs \u2013 what specific LLMs are used in conversational search?</p> <p>IBM watsonx Assistant uses large language models trained and deployed in watsonx for a variety of use cases, including conversational search. Assistant customizes watsonx LLMs for conversational search such that, given a question and a list of search results, they know how to re-rank the search results and then generate an answer from that list of search results. The watsonx Assistant team is working with IBM Research and the watsonx team to develop a custom, fine-tuned, and prompt-engineered large language model called Content-Grounded Assistant (CoGA) that specializes in generating answers from search results. CoGA is available in watsonx tech preview today. It will be available in watsonx later in Q3 2023.</p> <p>In the meantime, while the watsonx Assistant team works to make CoGA available on watsonx, the watsonx Assistant team recommends using prompt-engineered FLAN-UL2-20b, an open- source model, to generate answers from search results. FLAN-UL2-20b is available in watsonx today.</p> <p>Non-watsonx LLMs can also be fine-tuned or prompt-engineered to perform conversational search. These LLMs can be integrated with watsonx Assistant via extensions. This pattern may be preferable to clients who train their own LLMs in-house.</p> <p>The value-add of watsonx in conversational search is its customized large language model, especially CoGA, designed and customized by the watsonx Assistant, IBM Research, and watsonx teams to perform well for conversational search.</p>"},{"location":"lab-generative-ai-and-rag/#demonstrating-conversational-search","title":"Demonstrating conversational search","text":"<p>Following are the options available to IBMers and IBM business partners that would like to demo conversational search:</p> <p></p>"},{"location":"lab-generative-ai-and-rag/#search-tool","title":"Search tool","text":"<p>In today's lab we will explore the following method for search and retrieval:</p> <p>watsonx Discovery: This corresponds to semantic search method in a RAG architecture. This implementation will make use of an Elasticsearch instance as knowledge base, that has already been deployed for the purpose of this lab.</p> <ul> <li> <p>Semantic search, the \u201cmodern\u201d approach, is the process finding an answer to a question by means of a context-based method.</p> </li> <li> <p>Helps you find data based on the intent and contextual meaning of a search query, instead of a match on query terms.</p> </li> <li> <p>The semantic search approach requires us to teach machines to understand and process the meaning of text.</p> </li> <li> <p>This option requires to represent text passages as vectors, known as embeddings.</p> </li> <li> <p>IBM watsonx Discovery makes use of a semantic search method for the search and retrieval phase of the RAG architecture.</p> </li> </ul>"},{"location":"lab-generative-ai-and-rag/#1-connecting-to-the-environment","title":"[1] Connecting to the environment","text":"<ol> <li> <p>Open the watsonx.ai console in https://dataplatform.cloud.ibm.com/wx</p> </li> <li> <p>Make sure your account is itz-watsonx and the Region is Dallas.</p> <p></p> </li> </ol>"},{"location":"lab-generative-ai-and-rag/#2-create-a-watsonxai-project","title":"[2] Create a watsonx.ai project","text":"<p>IBM watsonx.ai is an enterprise-ready AI and data platform designed to multiply the impact of AI across a client\u2019s business. It provides an API for interacting with generative language models. In this step, you will create a watsonx project, which will later allow you to connect watsonx Assistant to the watsonx API.</p> <p>First, navigate to the watson.ai portal\u2019s main page. Then, create a new project by clicking the + button under Projects.</p> <p></p> <p>On the New project screen, 1. Enter a Name that is meaningful to you. 2. Optionally, enter a Description. 3. Select the Object storage service you created in a previous step. 4. Click Create.</p> <p></p> <p>Next, the Overview tab of your new watsonx project screen is shown.</p> <p>You will need the watsonx project ID to set up the action that calls the watsonx custom extension in Assistant later. To get the watsonx project ID, select the Manage tab, then click copy</p> <p> </p> <p>Save this project ID in a notepad for now, as you will need it shortly.</p> <p>Next, you will link the Watson Machine Learning service instance you created earlier to this new watsonx project. To do this, select Services &amp; integrations and click Associate service.</p> <p> </p> <p>On the next screen, select the Watson Machine Learning instance that was deployed for the purpose of this demo and click Associate.</p> <p></p> <p>Setting up your watsonx extension in Assistant will also require an API key from your IBM Cloud account. To get the API key, in a new browser tab, navigate to Manage access and users-API Keys in IBM Cloud, which will take you to the following screen. Then, click Create.</p> <p></p> <p>On the popup Create IBM Cloud API key screen, enter a Name and Description meaningful to you, then click Create.</p> <p></p> <p>When you see the notification API key successfully created, click Copy.</p> <p></p>"},{"location":"lab-generative-ai-and-rag/#3-load-data-into-watsonx-discovery","title":"[3] Load data into watsonx Discovery","text":"<p>Up until now, Watson Discovery was the service used for the creation of the Corporate Knowledge Base and the \u201cSearch and Retrieval\u201d phase of the conversational search integration. If you recall from the theory part of this course, this method corresponds to lexical search, since we are searching for lexical similarities in the stored documents.</p> <p>In this part of the laboratory, we want to make use of sematic search (instead of lexical) by connecting our current virtual agent with a vectorial database and retrieving the results based on the context and similarity of the passages instead of the traditional method used by Watson Discovery.</p> <p>For this reason, we are going to make use of the OpenAPI functionality to build our custom extension towards an ElasticSearch model. This model is known as ELSERv2 and has already been deployed to in an Elasticsearch service. In order to see where this ElasticSearch model is, open Elastic URL in a browser, authenticate with the following credentials:</p> <p>\u2192   Username: elastic</p> <p>\u2192   Password: J3723QrU9lX7Eld2E99NRT5D</p> <p>Then, go to Machine Learning and click on Trained Models.</p> <p></p> <p></p> <p></p> <p>The ELSER Model that has been deployed will be the component in the RAG architecture in charge of transforming the text passages (as well as the text query) into embeddings, for its use in semantic search. ELSER is an out-of-domain model which means it does not require fine-tuning on your own data, making it adaptable for various use cases out of the box. For more information, please refer to this link. Additionally, ElasticSearch has the ability to integrate with third-party or proprietary models. For the full list of Elastic supported models refer to this link.</p> <p>Download the following file:</p> <ul> <li>wa_docs_100.tsv</li> </ul> <p>Now we are going to load data into the Elasticsearch service. In order to do so go from the ElasticSearch main page to Machine Learning. Click on the section Data Vizualizer, then click on File. Click on \u201cSelect or drag and drop a File\u201d and select the wa_docs_100.tsv file you just downloaded. The file contains 100 passages of watsonx Assistant documentation. There are three columns in this TSV file, title, section_title and text. The columns are extracted from the original documents. Specifically, each text value is a small chunk of text split from the original document.</p> <p></p> <p></p> <p>Now, click on Override settings and then check Has header row checkbox because the example dataset has header row. Then, click close and import the data to a new Elasticsearch index and name it wa-docs-&lt;name&gt;, where &lt;name&gt; corresponds to your first name.</p> <p></p> <p></p> <p>We have the passages uploaded into elastic in the form of an index. However, if we want to use the semantic search framework, we need to transform the text passages into vector representation. For this reason, we create an index with mappings for ELSER output.  Execute from the bastion machine, that has been deployed for the purpose of the lab, the command to set as environment variables the URL, username and password.</p> <p>Info</p> <p>Note that you can copy the commands in this page by clicking the copy button in each code box:</p> <p></p> <p>Bastion password: ylGytzYJ</p> <pre><code>ssh admin@api.65e9b74cf3e1220011745c89.cloud.techzone.ibm.com -p 40222\n</code></pre> <pre><code>export ES_URL=https://elasticsearch-elastic.apps.65e9b74cf3e1220011745c89.cloud.techzone.ibm.com:443\nexport ES_USER=elastic\nexport ES_PASSWORD=J3723QrU9lX7Eld2E99NRT5D\n</code></pre> <p>Now, execute the following curl. Please edit the URL so that search-wa-docs-&lt;name&gt; contains your first name. </p> <pre><code>curl -X PUT \"${ES_URL}/search-wa-docs-&lt;NAME&gt;?pretty\" -u \"${ES_USER}:${ES_PASSWORD}\" \\\n-H \"Content-Type: application/json\" --insecure -d'\n{\n  \"mappings\": {\n    \"_source\": {\n        \"excludes\": [\n          \"text_embedding\"\n        ]\n    },\n    \"properties\": {\n      \"text_embedding\": {\n        \"type\": \"sparse_vector\"\n      },\n      \"text\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}'\n</code></pre> <p>Notes</p> <ul> <li>search-wa-docs-&lt;name&gt; will be your index name.</li> <li>text_embedding is the field that will keep ELSER output when data is ingested, and sparse_vector type is required for ELSER output field.</li> <li>text is the input filed for the inference processor. In the example dataset, the name of the input field is text which will be used by ELSER model to process.</li> </ul> <p>The output of the previous command should be something like: </p> <pre><code>{\n    \"acknowledged\" : true,\n    \"shards_acknowledged\" : true,\n    \"index\" : \"search-wa-docs-javier\"\n}\n</code></pre> <p>Now that we have created out destination index to which the embeddings will be uploaded, we follow by creating an ingest pipeline with an inference processor. This ingest pipeline will be in charge of populating the destination index with data from our watsonx Assistant documentation source index. To do so execute from bastion machine the following curl command. Please edit the URL so that pipeline name elser-v2-test-&lt;name&gt; contains your first name.</p> <pre><code>curl -X PUT \"${ES_URL}/_ingest/pipeline/elser-v2-test-&lt;NAME&gt;?pretty\" -u \"${ES_USER}:${ES_PASSWORD}\" \\\n-H \"Content-Type: application/json\" --insecure -d'\n{\n  \"processors\": [\n    {\n      \"inference\": {\n        \"model_id\": \".elser_model_2\",\n        \"input_output\": [\n          {\n            \"input_field\": \"text\",\n            \"output_field\": \"text_embedding\"\n          }\n        ]\n      }\n    }\n  ]\n}'\n</code></pre> <p>The output of the previous command should be something like: </p> <pre><code>{\n    \"acknowledged\" : true\n}\n</code></pre> <p>Execute the following curl command for the ingestion to start. This command creates the tokens from the text by reindexing the data through the inference pipeline that uses ELSER as the inference model. Edit the curl so that the variable &lt;name&gt; corresponds to your first name.</p> <pre><code>curl -X POST \"${ES_URL}/_reindex?wait_for_completion=false&amp;pretty\" -u \"${ES_USER}:${ES_PASSWORD}\" \\\n-H \"Content-Type: application/json\" --insecure -d'\n{\n  \"source\": {\n    \"index\": \"wa-docs-&lt;name&gt;\"\n  },\n  \"dest\": {\n    \"index\": \"search-wa-docs-&lt;NAME&gt;\",\n    \"pipeline\": \"elser-v2-test-&lt;NAME&gt;\"\n  }\n}'\n</code></pre> <p>Notes</p> <ul> <li>wa-docs-&lt;name&gt; is the index you created when uploading the example file to Elasticsearch cluster. It contains the text data.</li> <li>search_wa-docs-&lt;name&gt; is the search index that has ELSER output field.</li> <li>elser-v2-test-&lt;name&gt; is the ingest pipeline with an inference processor using ELSER model.</li> </ul> <p>The output of the previous command should be something like: </p> <pre><code>{\n    \"task\" : \"nkJhgkDtRTyQ05BcWI0o5g:87752\"\n}\n</code></pre> <p>Go in the Elasticsearch web interface to Content, then click on Indeces and wait for the search-wa-docs-&lt;name&gt; index appears with 100 Docs count.</p> <p></p> <p>Once the index is available, we can test the semantic search index, by executing the following curl command. Make sure that search-wa-docs-&lt;name&gt; corresponds to your previously created index.</p> <pre><code>curl -X GET \"${ES_URL}/search-wa-docs-&lt;NAME&gt;/_search?pretty\" -u \"${ES_USER}:${ES_PASSWORD}\" -H \"Content-Type: application/json\" --insecure -d'\n{\n   \"query\":{\n      \"text_expansion\":{\n         \"text_embedding\":{\n            \"model_id\":\".elser_model_2\",\n            \"model_text\":\"how to set up custom extension?\"\n         }\n      }\n   }\n}'\n</code></pre> <p>We are now ready to build the extension in watsonx Assistant for Elasticsearch API.</p>"},{"location":"lab-generative-ai-and-rag/#4-set-up-the-assistant","title":"[4] Set up the assistant","text":"<p>From the watsonx Assistant instance launch page, click Launch watsonx Assistant.</p> <p></p> <p>This is a new assistant, so you will first configure the basics. First, enter its name, description, and click Next.</p> <p></p> <p></p> <p>Customize the assistant to your linking, then click Next and Create.</p> <p></p>"},{"location":"lab-generative-ai-and-rag/#5-adding-the-watsonx-discovery-semantical-search-extension","title":"[5] Adding the watsonx Discovery Semantical Search Extension","text":"<p>Download the OpenAPI specification for the watsonx Discovery integration (right-click in the following link -&gt; download):</p> <ul> <li>elasticsearch-generic-openapi.json</li> </ul> <p>Use the OpenAPI specification to build a custom extension. Go to the Integrations page.</p> <p></p> <p>Then, click on Build custom extension.</p> <p></p> <p>Click on Next.</p> <p></p> <p>Enter the Extension name: watsonx Discovery, and click Next.</p> <p></p> <p>Add the OpenAPI specification, and click Next.</p> <p></p> <p>Review the extension and click Finish.</p> <p></p> <p>You have created the watsonx Discovery custom extension, and now you need to specify which watsonx Discovery instance it will access. On the popup window, click Add.</p> <p></p> <p></p> <p>Click Next and configure the Authentication:</p> <ul> <li>Authentication type: Basic Auth</li> <li>Username: elastic</li> <li>Password: J3723QrU9lX7Eld2E99NRT5D</li> <li>elastic_url: elasticsearch-elastic.apps.65e9b74cf3e1220011745c89.cloud.techzone.ibm.com:443</li> </ul> <p></p> <p>Click Next and the Finish.</p> <p></p> <p>Why am I not using the watsonx Discovery built-in search extension?</p> <p>An alternative to creating a custom Watsonx Discovery extension is to use the built-in search integration (also referred to as the Watson Assistant search skill). The search skill is a purpose- built integration between Assistant and Discovery for a simple, quicker, and easier experience.</p> <p>However, it does not expose and use all of the API parameters and the full power of watsonx Discovery, which will be used by watsonx. For example, the built-in search skill has an \"Emphasize the Answer\" capability which allows Watson Assistant to identify the snippet of text within each response that it thinks is most likely to be the direct, concise answer to the  question that was asked and will emphasize that text within the search result card. That can be a great feature when not using LLMs, but it can be detrimental when integrating with an LLM, especially if the result has no useful answer. </p> <p>Furthermore, watsonx Discovery provides an answer confidence for each answer that provides an estimate of the probability that the answer is correct, so a user of watsonx Discovery can use that probability to decide whether the answer is plausible enough that it should be emphasized. This value corresponds with the semnatic similarity between the query vector and the retrieved stored vector. The built-in search integration in Watson Assistant does not expose the answer confidence because it is designed to provide simple, minimal functionality. In contrast, a custom extension lets you access all of the outputs of watsonx Discovery, so that low confidence answers can be ignored.</p>"},{"location":"lab-generative-ai-and-rag/#6-adding-the-watsonxai-custom-extension","title":"[6] Adding the watsonx.ai custom extension","text":"<p>Next, you will create the watsonx custom extension. First, download the watsonx OpenAPI specification file from the following link:</p> <ul> <li>watsonx-openapi.json</li> </ul> <p>This JSON file defines the watsonx custom extension.</p> <p>In watsonx Assistant, use the left menu to open the Integrations page. Then, scroll down and click the Build custom extension button.</p> <p></p> <p>This first screen describes what you\u2019ll need to create the extension. Note that the watsonx OpenAPI JSON document, which you just obtained, is key to the setup. Click Next in the top right to proceed to the next screen.</p> <p></p> <p>The second screen asks you to name and describe the custom extension. Name it watsonx custom extension and add a description, like \u201cCustom extension for watsonx\u201d. Click Next to proceed to the next screen.</p> <p></p> <p>In the next screen, either drag-and-drop or click to upload the watsonx OpenAPI specification file OpenAPI spec you downloaded, then click Next to proceed to the next screen.</p> <p></p> <p>Click Finish to create the custom extension.</p> <p></p> <p>You should now be able to see the watsonx custom extension in your Integrations catalog. Click Add so that you can configure a connection to your watsonx project.</p> <p></p> <p>Click Add on the popup screen and hit Next on the following screen.</p> <p>On the Authentication screen configure the following parameters: \u2022   Choose OAuth 2.0 as the Authentication type. \u2022   Select Custom apikey as the Grant type. \u2022   Copy and paste the IBM Cloud API key you saved earlier into the Apikey field. \u2022   Click Next.</p> <p></p> <p>Click Finish and Close on the final review screen.</p>"},{"location":"lab-generative-ai-and-rag/#6-upload-and-configure-the-watsonx-actions","title":"[6] Upload and configure the watsonx actions","text":"<p>Next, you will upload the actions your assistant will need, so download the actions JSON file.</p> <p>Note: You should not upload these actions directly into an existing assistant because doing so will overwrite your existing actions.</p> <p>The actions in this file will use search only when no action matches the user request. They search a complete watsonx project, and as such they are general-purpose and usable with any data set.</p> <p>To upload the JSON file, click Actions, and then Global Settings.</p> <p></p> <p></p> <p>Scroll to the right until you are able to see and select the Upload/Download tab. There, drag and drop the actions JSON file, and click Upload.</p> <p></p> <p></p> <p>Click Upload and upload the Gen-AI-Workshop-action file that you can download from the following link:</p> <ul> <li>Gen-AI-Workshop-action.json</li> </ul> <p>Then, click Close.</p> <p>This file contains three actions:</p> <ul> <li> <p>Search: Connects to watsonx Discovery to search for documents related to the user query. The out-of-box \"No Action Matches\" action has been configured to send all input to this action, so whatever the user enters will be used as the search input. In turn, this action invokes the \"Generate Answer\" action to generate a response to the query.</p> </li> <li> <p>Generate answer: Configures the query prompt and document passages resulting from the Search action and calls the action \"Invoke watsonx generation API.\" It is not meant to be invoked directly, but rather by the \"Search\" action.</p> </li> <li> <p>Invoke watsonx generation API: Connects to watsonx and, using as context the documents resulting from the search, asks the language model to generate an answer to the user query. It is not meant to be invoked directly, but rather by the \"Generate Answer\" action.</p> </li> </ul> <p>The actions JSON file you uploaded also includes variables used by the actions. You need to update two of them with your watsonx Discovery text embeddings index and watsonx project ID.</p> <p>First, click on the index_name variable and set the Initial value to your index name. It should be something like search-wa-docs-&lt;name&gt;. Then, click Save.</p> <p></p> <p></p> <p>Now, click on the variable watsonx_project_id and set the Initial value to the watsonx.ai\u2019s project ID. Then, click Save.</p> <p></p> <p></p> <p>This list describes the available variables providing for greater control over your model:</p> <ol> <li>model_id: The id of the watsonx model that you select for this action. Defaults to google/flan-ul2.</li> <li>model_input: The input to the watsonx model. You may change this to do prompt engineering, but a default will be used by the model if you don\u2019t pass a prompt here.</li> <li>model_parameters_max_new_tokens: The maximum number of new tokens to be generated. Defaults to 300.</li> <li>model_parameters_min_new_tokens: The minimum number of the new tokens to be generated. Defaults to 1.</li> <li>model_parameters_repetition_penalty: Represents the penalty for penalizing tokens that have already been generated or belong to the context. The range is 1.0 to 2.0 and defaults to 1.1.</li> <li>model_parameters_stop_sequences: Stop sequences are one or more strings which will cause the text generation to stop if/when they are produced as part of the output. Stop sequences encountered prior to the minimum number of tokens being generated will be ignored. The list may contain up to 6 strings. Defaults to [\"\\n\\n\"]</li> <li>model_parameters_temperature: The value used to control the next token probabilities. The range is from 0 to 1.00; 0 makes it deterministic. model_response: The text generated by the model in response to the model input.</li> <li>passages: Concatenation of top search results.</li> <li>query_text: You may change this to pass queries to watsonx Discovery. By default the Search action passes the user\u2019s input.text directly.</li> <li>search_results: Response object from Discovery query.</li> <li>snippet: Top results from the watsonx Discovery document search.</li> <li>verbose: A boolean that will print debugging output if true. Default is false.</li> <li>watsonx_api_version: watsonx api date version. It currently defaults to 2023-05-29.</li> <li>watsonx_project_id: You must set this value to be a project ID value from watsonx.</li> <li>index_name: the name of the index in watsonx Discovery.</li> </ol> <p>Try it out!</p> <p>Now that you are finished configuring your assistant, try it out! Click on Preview and enter the same question as before: what is a session variable?. Note the clear, concise, and conversational answer. Compare this to the generic answer from ChatGPT and the excerpt answer provided by Watson Discovery earlier!</p> <p></p> <p></p> <p>The End</p> <p>You have reached the end of this lab.</p>"},{"location":"lab-tuning-generative-ai/","title":"Lab 2. Tuning Gen AI","text":""},{"location":"lab-tuning-generative-ai/#tuning-generative-ai-prompt-engineering-vs-prompt-tuning","title":"Tuning Generative AI: Prompt Engineering vs Prompt Tuning","text":""},{"location":"lab-tuning-generative-ai/#in-this-session","title":"In this session","text":"<p>We are going to work on a use case where prompt engineering has a hard time getting the desired outputs. On the other hand, we will apply prompt tuning to analyze the results generated. For this, we will use Prompt Lab to perform prompt engineering and evaluate the model outputs, and on the other hand, Tuning Studio to run the prompt tuning experiment, creating a tuned version of an existing foundation model. These two resources are available at IBM watsonx.ai.</p>"},{"location":"lab-tuning-generative-ai/#a-classification-use-case","title":"A classification use case","text":"<p>Consider the following classification use case. Suppose you are tasked with automating the triage of incoming complaints (a classification task). For your company, you need to route a complaint to all applicable departments among the following:</p> <ul> <li>Planning</li> <li>Development</li> <li>Service</li> <li>Warehouse</li> <li>Level 3 (L3 for short)</li> <li>Packaging</li> <li>Marketing</li> </ul> <p>Some business rules exist for each department. Some of these include:</p> <ul> <li>Service/Support complaints go to Service. </li> <li>Skills issues that need support go to Service and L3. </li> <li>Out-of-stock/missing items complaints go to Warehouse and may involve Packaging.</li> <li>If the item is being discontinued, the complaints go to Warehouse and Planning.</li> <li>Missing feature requests go to Planning and Development. </li> <li>Complaints that are related to the perception of what the business does or does not provide go to Marketing and may involve Planning and Development.</li> <li>And more \u2026 based on your business process.</li> </ul> <p>This is a good generative AI use case as a large enterprise must handle lots of complaints and they must be routed properly to the correct department for rapid response. Such routing tasks need to be automated without spending a large number of human resources. Generative AI is also better at handling incoming complaints in natural language.</p> <p>However, there are complexities with this use case. Large Language Models (LLMs) are very good at classifying with single labels (such as positive vs. negative sentiments) but will have a tougher time with business categories which may have very different meanings than the training data for the LLM. </p> <p>In this case, sometimes the business requires multiple labels output, and there is specific business logic for why a particular output label is used. Prompt engineering, as you will discover, will have a tough time properly classifying all complaints. In prompt tuning, instead of human-generated hard prompts, a user provides a set of labeled data to tune the model. For this example, you will provide a list of sample complaints and the associated departments to be notified. Watsonx.ai will tune the model using this data and create a \u201csoft prompt\u201d. This does not change any of the existing model weights. Instead, when a user enters a prompt, it is augmented by the soft prompt and is passed to the model to generate output.</p> <p>Next, we can see a series of examples of the training dataset (in JSONL format) that we will use to execute prompt tuning:</p> <p></p>"},{"location":"lab-tuning-generative-ai/#connecting-to-the-environment","title":"Connecting to the environment","text":"<ol> <li> <p>Click on this link to access the watsonx platform.</p> </li> <li> <p>Onces the watsonx console is opened, make sure to select the IBM Cloud account where you were invited, and the correct location (Dallas). </p> <p></p> </li> </ol>"},{"location":"lab-tuning-generative-ai/#create-a-watsonxai-project-and-deployment-space","title":"Create a watsonx.ai project and deployment space","text":"<ol> <li> <p>From the watsonx.ai console (Home window), click + to the right of Projects to create a new project.</p> <p></p> </li> <li> <p>Name the new project as &lt;Your Initials&gt; Tuning Workshop (where &lt;Your Initials&gt; is your name initials [i.e. John Smith would be JS]). Make sure the storage service is automatically selected. Then, click Create.</p> <p></p> </li> <li> <p>Next, you will link the Watson Machine Learning service to this watsonx project. To do this, move to the Manage window of the project. Select Services &amp; Integrations and click Associate service +.</p> <p></p> </li> <li> <p>On the next screen, select the Watson Machine Learning instance that is provided throught TechZone.</p> <p></p> </li> <li> <p>Once associated, let's go back to the watsonx.ai console by opening the left side window and clicking Home.</p> <p></p> </li> <li> <p>Click + to the right of Deployment spaces to create a new deployment space.</p> <p></p> </li> <li> <p>Name the new deployment space &lt;Your Initials&gt; Tuning Space. Select Development as Deployment stage. Make sure a storage service is selected (should be automatically selected when using a TechZone reservation). Select the machine learning service available with the TechZone reservation. Click Create.</p> <p></p> </li> <li> <p>Wait until the deployment space is created and then click Close.</p> <p></p> </li> <li> <p>Once created, let's go back to the watsonx.ai console by opening the left side window and clicking Home.</p> <p></p> </li> </ol>"},{"location":"lab-tuning-generative-ai/#prompt-engineering-vs-prompt-tuning-with-flan-models","title":"Prompt engineering vs Prompt tuning with Flan models","text":"<p>Let's start by doing prompt engineering with the small flan model, flan-t5-xl-3b. Then we will perform some tests with the large flan model, flan-ul2-20b. Finally, we will run a prompt tuning experiment on flan-t5-xl-3b, and perform an evaluation of the outputs in the prompt lab.</p>"},{"location":"lab-tuning-generative-ai/#download-the-training-dataset","title":"Download the training dataset","text":"<ol> <li> <p>Download the following dataset. Use right-click -&gt; download to save the file in your workstation:</p> <ul> <li>Call center complaints.jsonl</li> </ul> </li> </ol>"},{"location":"lab-tuning-generative-ai/#running-prompt-tuning-experiment","title":"Running Prompt tuning experiment","text":"<ol> <li> <p>From the Home window, open the new project previously created by clicking in Tuning Workshop.</p> <p></p> </li> <li> <p>First of all, we're going to start the execution of prompt tuning with flan-t5-xl-3b while we perform prompt engineering. In the Overview window, let's open the tuning studio by clicking Tune a foundation model with labeled data.</p> <p></p> </li> <li> <p>Name the tuning experiment as Tuning flan-t5-xl-3b and then click Create.</p> <p></p> </li> <li> <p>Click Select a foundation model.</p> <p></p> </li> <li> <p>Select flan-t5-xl-3b model. </p> <p></p> </li> <li> <p>Confirm model selection by clicking Select.</p> <p></p> </li> <li> <p>Select Text as the initialization method and copy and paste the following as the initial prompt.</p> <pre><code>Classify the following complaint and determine which departments to route the complaint: Planning, Development, Service, Warehouse, L3, Packaging, and Marketing.\n</code></pre> <p>Info</p> <p>You can use the copy button that appears in the right side of the code box as seen in this screenshot:</p> <p></p> <p></p> </li> <li> <p>Select Generation as the task that fits our goal. We pick generation instead of classification because we need the LLM to output completion with multiple classifications. Generation is able to generate text in a certain style and format. The model will be trained with labeled data to output the proper completion.</p> <p></p> </li> <li> <p>The Add training data column appears. You can use the Browse button to add the Call center complaints.jsonl file.</p> <p></p> </li> <li> <p>Select the Call center complaints.jsonl file. </p> <p></p> <p>watsonx.ai will perform a quick verification check on the file. If there is any error message, you will need to fix the JSONL file and re-load the file. </p> </li> <li> <p>Now, let's click on Configure parameters.</p> <p></p> </li> <li> <p>The Configure parameters page opens. For now, there is no need to modify the parameters, but it is important to take into account which are the default values. Click Cancel to exit this page.</p> <p></p> </li> <li> <p>Click Start tuning.</p> <p></p> </li> <li> <p>You are returned to the Tuning experiment. Note that this can take quite some time (between 5-10 minutes).</p> <p></p> </li> </ol>"},{"location":"lab-tuning-generative-ai/#performing-prompt-engineering-optional","title":"Performing Prompt engineering (OPTIONAL)","text":"<p>While we wait for the prompt tuning experiment to finish, we are going to perform prompt engineering with the flan models and evaluate the outputs obtained in each case.</p> <ol> <li> <p>Go back to the project by clicking Tuning Workshop on the top left. Then, go to Overview and click Experiment with foundation models and build prompts.</p> <p></p> </li> <li> <p>If the Prompt lab opens with the Structure mode, change to Freeform. Then, select the flan-t5-xl-3b model (open the foundation models library if required).</p> <p></p> </li> <li> <p>Let's start by performing zero-shot prompting. Copy and paste the next prompt and click Generate.</p> <pre><code>Classify the following complaint and determine which departments to route the complaint: Planning, Development, Service, Warehouse, L3, Packaging, and Marketing.\n\nWhere are the 2 extra sets of sheets that are supposed to come with my order?\n</code></pre> <p></p> <p>Given the business use case and the background information, you want the model to respond with a completion of Warehouse and Packaging, but instead, the model returns a completion of Marketing. This is not an unreasonable answer given that the flan model has no understanding of the business context.</p> <p>For now, note the following token costs (you will use this information later):</p> <p>Tokens: 47 input + 2 generated = 49 out of 4096</p> </li> <li> <p>Now, we'll do a one-shot prompting by adding a single example. Copy and paste the next prompt to the freeform, deleting the previous one. Then, click Generate.</p> <pre><code>Classify the following complaint and determine which departments to route the complaint: Planning, Development, Service, Warehouse, L3, Packaging, and Marketing.\n\nInput: I was put on hold for 2 hours and your so-called SME cannot answer my questions!\nOutput: Service, L3\n\nInput: Where are the 2 extra sets of sheets that are supposed to come with my order?\nOutput: \n</code></pre> <p></p> <p>There is some improvement over the zero-shot prompt in that the LLM identifies Warehouse this time. Even so, it does not return the desired second class.</p> <p>Note the following token costs (you will use this information later):</p> <p>Tokens: 84 input + 2 generated = 86 out of 4096</p> </li> <li> <p>Let's try to achieve the desired output by performing a 3-shot prompting. Copy and paste the next prompt to the freeform, deleting the previous one. Then, click Generate.</p> <pre><code>Classify the following complaint and determine which departments to route the complaint: Planning, Development, Service, Warehouse, L3, Packaging, and Marketing.\n\nInput: I was put on hold for 2 hours and your so-called SME cannot answer my questions!\nOutput: Service, L3\n\nInput: I cannot find the mouthguard to the hockey set. It is useless without it.\nOutput: Warehouse, Packaging\n\nInput: The Kron model you shipped me is missing 2 drawer handles!\nOutput: Warehouse, Packaging\n\nInput: Where are the 2 extra sets of sheets that are supposed to come with my order?\nOutput: \n</code></pre> <p></p> <p>The model has not improved its completion and still just provides Warehouse as the completion. The model has not learned to identify more than one class despite the 3-shot prompt. </p> <p>We could try additional shots. Given enough examples that look very similar to the prompt that we are trying to classify, we could get the flan-t5-xl-3b to repeat Warehouse and Packaging as the completion. But that just means the LLM can recognize this pattern. It is not likely to work on other patterns.</p> <p>Note the following token costs (you will use this information later):</p> <p>Tokens: 132 input + 2 generated = 134 out of 4096</p> </li> <li> <p>Finally, let's try this same 3-shot prompting with the large flan model. Select the flan-ul2-20b model. We need to delete the previous generated output, keeping the rest of the prompt. Click Generate.</p> <p></p> <p>Even with a larger model, it does not seem to learn that multiple targets for notification are allowed and desirable. It is still incorrect, despite every example passed in having multiple departments in the output.</p> </li> </ol>"},{"location":"lab-tuning-generative-ai/#deploy-and-evaluate-tuned-flan-model","title":"Deploy and evaluate Tuned Flan model","text":"<ol> <li> <p>Go back to the project by clicking Tuning Workshop on the top left. If a pop up appears, click Leave.</p> <p></p> </li> <li> <p>Then, in the Assets window, click on Tuning flan-t5-xl-3b experiment.</p> <p></p> </li> <li> <p>The tuning experiment should be completed by now. First of all, observe the loss function obtained in the experiment.</p> <p></p> <p>Remember that the evaluation is an iterative process, at this point, we would consider the possibility to run more experiments with other parameters to try to get as close as possible to 0. Anyway, the model is no longer gaining a lot more knowledge, since the loss function levels off approximately at 0.7 around epoch 15. In this case, we could try to slightly increase the learning rate and rerun the experiment. However, this is sufficient for the current lab.</p> <p>According to the evaluation process, after we try to improve the loss function by adjusting parameters or improving the data set, the next step is to evaluate the outputs obtained by the tuned model. </p> </li> <li> <p>The tuned model needs to be deployed before it can be used. Scroll down on the Tuning experiment page and click on New deployment.</p> <p></p> </li> <li> <p>The Deploy the tuned model page opens. Notice that the Name is Tuning flan-t5-xl-3b (1). You can add an additional Description or Tags. Select the Deployment space named Tuning Space, that we created in the previous steps. Then, click Create.</p> <p></p> <p>You get this message:</p> <p></p> </li> <li> <p>When completed, you will see that your tuned model is deployed. Click on this Tuning flan-t5-xl-3b (1) model.</p> <p></p> </li> <li> <p>Click on the Open in the Prompt Lab pulldown and select the project you want to use. In the example below, it is Project: Tuning Workshop.</p> <p></p> </li> <li> <p>The watsonx.ai Prompt Lab page opens and the Tuning flan-t5-xl-3b (1) model is automatically selected. Make sure you are using Freeform.</p> <p></p> </li> <li> <p>Let's start by inferencing one complaint with the Instruction text. Copy and paste the next prompt and click Generate.</p> <pre><code>Classify the following complaint and determine which departments to route the complaint: Planning, Development, Service, Warehouse, L3, Packaging, and Marketing.\n\nWhere are the 2 extra sets of sheets that are supposed to come with my order?\n</code></pre> <p></p> <p>And that completion of Warehouse, Packaging is exactly what you want to see.</p> <p>Note the following token costs (you will use this information later):</p> <p>Tokens: 53 input + 4 generated = 57 out of 4096</p> </li> <li> <p>Now, copy and paste the next sentence alone, deleting the previous prompt. Then, click Generate.</p> <pre><code>Where are the 2 extra sets of sheets that are supposed to come with my order?\n</code></pre> <p></p> <p>Same result. You do not need to provide any text for Instruction as that information was already included when you did the prompt tuning. This is another advantage of prompt tuning over prompt engineering. With no need for Instruction, fewer tokens are consumed every time this tuned model is used. This makes it easier to use, and the cost savings will add up.</p> <p>Note the following token costs (you will use this information later):</p> <p>Tokens: 24 input + 4 generated = 28 out of 4096</p> </li> <li> <p>Let's try another complaint and see if the output makes sense. Copy and paste the following sentence. Then, click Generate.</p> <pre><code>I see a 2-door model in your TV ad, but why is that not available?\n</code></pre> <p></p> <p>The Tuning flan-t5-xl-3b (1) model returns with a completion of Planning, Marketing. This is what you expected according to the business rules. The fact that it appears in the TV advertisement but is not available can be a marketing mistake. On the other hand, if this is truly a missing feature then clearly customers are looking for it, so Planning should be notified.</p> </li> <li> <p>Finally, we try one last complaint to evaluate the generated output. Copy and paste the following sentence. Then, click Generate.</p> <pre><code>I could not get someone on the phone who could fix my problems! Your so-called \"SMEs\" are just not helpful.\n</code></pre> <p></p> <p>The Tuning flan-t5-xl-3b (1) model returns with the completion of Service, L3. This is again what is expected according to the business rules. This is clearly a Service issue, but with the comment on SME, the Level 3 support team needs to be notified as well.</p> </li> <li> <p>Compare the previously noted token usage information for the same query: \"Where are the 2 extra sets of sheets that are supposed to come with my order?\".</p> <p></p> <p>This shows another advantage of Prompt tuning. The inference using the prompt tuned model without instruction, is significantly less costly (in therms of tokens consumed) than the 3-shot prompting, which didn't even work. Like we commented in previous steps, we would need to increase the number of examples to make the prompt engineering work at least for one query, which would already increase the use of tokens.</p> </li> </ol>"},{"location":"lab-tuning-generative-ai/#prompt-engineering-vs-prompt-tuning-with-llama-models","title":"Prompt engineering vs Prompt tuning with Llama models","text":"<p>In this section, we are going to perform similar experiments but using the models from the llama family. Let's start by doing prompt engineering directly with the large llama model, llama-2-70b-chat, which is the largest model in the watsonx.ai family. Then, we will run a prompt tuning experiment on the small version of the model, llama-2-13b-chat, and perform an evaluation of the outputs in the prompt lab.</p>"},{"location":"lab-tuning-generative-ai/#performing-prompt-engineering-optional_1","title":"Performing Prompt engineering (OPTIONAL)","text":"<ol> <li> <p>Select the llama-2-70b-chat model (open the foundation models library if required). We'll start with zero-shot prompting. In the Freeform mode, copy and paste the following prompt. Then, click Generate.</p> <pre><code>Classify the following complaint and determine which departments to route the complaint: Planning, Development, Service, Warehouse, L3, Packaging, and Marketing.\n\nWhere are the 2 extra sets of sheets that are supposed to come with my order?\n</code></pre> <p></p> <p>The llama-2-70b-chat model has not understood the task, as it consists in returning the departments where the complaint should be routed. Instead, it has returned an extension of the complaint itself, along with two fields to be filled in: classification and department to route complaint.</p> <p>Note the following token costs (you will use this information later):</p> <p>Tokens: 59 input + 75 generated = 134 out of 4096</p> </li> <li> <p>Let's add one example (one-shot prompting) to show the model the output we are looking for. Copy and paste the following prompt. Then, click Generate.</p> <pre><code>Classify the following complaint and determine which departments to route the complaint: Planning, Development, Service, Warehouse, L3, Packaging, and Marketing.\n\nInput: I was put on hold for 2 hours and your so-called SME cannot answer my questions!\nOutput: Service, L3\n\nInput: Where are the 2 extra sets of sheets that are supposed to come with my order?\nOutput: \n</code></pre> <p></p> <p>As we can see, the model starts returning a loop pattern of input/output pairs. The first line that returns, responds to the input that we have introduced, and it does it correctly, with the labels that we expected to obtain. If you try this same prompt with llama-2-13b-chat, you'll observe this same behaviour. </p> <p>Note the following token costs (you will use this information later):</p> <p>Tokens: 97 input + 200 generated = 297 out of 4096</p> </li> <li> <p>To limit ourselves to obtain the desired output, we use the Enter key as stop sequence in the model parameters. To do that, we open the Model parameters window and add the Enter key to the stop sequences. Next, delete the generated output, keeping the previous prompt. Then, click Generate.</p> <p></p> <p>Choosing effective stop sequences depends on your use case and the nature of the generated output that you expect. We haven't really made the model understand the outputs we want, but we have managed to constrain it to do so.</p> <p>Note the following token costs (you will use this information later):</p> <p>Tokens: 97 input + 7 generated = 104 out of 4096</p> </li> <li> <p>Let's try another complaint and see if the output makes sense. Copy and paste the following sentence. Then, click Generate.</p> <pre><code>Classify the following complaint and determine which departments to route the complaint: Planning, Development, Service, Warehouse, L3, Packaging, and Marketing.\n\nInput: I was put on hold for 2 hours and your so-called SME cannot answer my questions!\nOutput: Service, L3\n\nInput: I see a 2-door model in your TV ad, but why is that not available?\nOutput: \n</code></pre> <p></p> <p>The generated output for this complaint, Marketing, Planning, is what we expected. </p> </li> <li> <p>Finally, we try one last complaint to evaluate the generated output. Copy and paste the following sentence. Then, click Generate.</p> <pre><code>Classify the following complaint and determine which departments to route the complaint: Planning, Development, Service, Warehouse, L3, Packaging, and Marketing.\n\nInput: I was put on hold for 2 hours and your so-called SME cannot answer my questions!\nOutput: Service, L3\n\nInput: I could not get someone on the phone who could fix my problems! Your so-called \"SMEs\" are just not helpful.\nOutput: \n</code></pre> <p></p> <p>The generated output for this complaint, Service, L3, is what we expected.</p> <p>As we mentioned at the beginning of the lab, this use case is complex to solve for prompt engineering, because there is a specific business logic for each of the labels and they may have a different meaning than the LLM training data. Even so, we have managed to obtain satisfactory outputs with the Llama family of models and specifically with a sufficiently large model, although we have had to intervene by introducing a stop sequence. </p> <p>It should be noted that we have evaluated the output of only three different complaints, where the results have been as desired. Since we are dealing with prompt engineering, it is possible to find other cases with a specific business logic that does not work, which would require adding more examples. Even so, in general terms we can conclude that the results are satisfactory.</p> <p>Next, we will run a tuning experiment of the small llama model, llama-2-13b-chat, and we will discuss some key points to consider when deciding between prompt engineering or prompt tuning in this case.</p> </li> </ol>"},{"location":"lab-tuning-generative-ai/#running-prompt-tuning-experiment-optional","title":"Running Prompt tuning experiment (OPTIONAL)","text":"<p>This section is optional, since the running time of the experiment with the llama-2-13b-chat model is between 15 and 20 minutes. </p> <ol> <li> <p>Go back to the project by clicking Tuning Workshop on the top left. If a pop up appears, click Leave.</p> <p></p> </li> <li> <p>Follow the steps 2 to 14 of the Prompt engineering vs Prompt tuning with Flan models section, but selecting the llama-2-13b-chat model instead of the flan-t5-xl-3b model, and naming the tuning experiment accordingly.</p> </li> <li> <p>Wait until the tuning experiment is completed. Then, observe the loss function obtained.</p> <p></p> <p>In this case, the minimum value reached in 20 epochs is 0.374, and it seems to have a tendency to continue decreasing with some more epochs. A good option would be to increase slightly the learning rate, since the loss curve does not drop significantly. For this lab, this result is sufficient, so we move on to the deploy and output evaluation.</p> </li> </ol>"},{"location":"lab-tuning-generative-ai/#deploy-and-evaluate-tuned-llama-model-optional","title":"Deploy and evaluate Tuned Llama model (OPTIONAL)","text":"<p>Perform the next steps only if you executed and completed the tuning experiment of the llama-2-13b-chat model. Otherwise, you can still scheck this section to observe the results.</p> <ol> <li> <p>Follow the steps 23 to 27 of the Prompt engineering vs Prompt tuning with Flan models section, in order to deploy the model and open the prompt lab with the new tuned llama model.</p> </li> <li> <p>Let's start by inferencing one complaint directly without Instruction. Copy and paste the next prompt and click Generate.</p> <pre><code>Where are the 2 extra sets of sheets that are supposed to come with my order?\n</code></pre> <p></p> <p>The generated output for this complaint, Warehouse, Packaging, is what we expected.</p> <p>Note the following token costs (you will use this information later):</p> <p>Tokens: 19 input + 8 generated = 27 out of 4096</p> </li> <li> <p>Try the second complaint to check the generated output. Copy and paste the next prompt and click Generate.</p> <pre><code>I see a 2-door model in your TV ad, but why is that not available? \n</code></pre> <p></p> <p>The generated output for this complaint, Marketing, Planning, is what we expected.</p> </li> <li> <p>Finally, try the third complaint. Copy and paste the next prompt and click Generate.</p> <pre><code>I could not get someone on the phone who could fix my problems! Your so-called \"SMEs\" are just not helpful. \n</code></pre> <p></p> <p>The generated output for this complaint, Service, L3, is what we expected.</p> </li> </ol>"},{"location":"lab-tuning-generative-ai/#prompt-engineering-or-prompt-tuning-which-one-to-choose","title":"Prompt engineering or Prompt tuning, which one to choose?","text":"<p>We have achieved satisfactory outputs for both prompt engineering and prompt tuning with the llama model family. In the following, we mention a number of key points that will help us to decide which method to use in cases like this.</p> <ul> <li>Token costs for each inference. The next table contains a comparison of the previously noted token usage information for the same query: \"Where are the 2 extra sets of sheets that are supposed to come with my order?\".</li> </ul> <p></p> <p>Thanks to prompt tuning, we have reduced the number of tokens for each inference (more than half) with respect to the one-shot prompting with stop sequence. It should be noted that this is a cost saving in the long term, since with each inference performed we are saving a considerable number of tokens compared to prompt engineering.</p> <p>The use of tokens translates into economic cost. The Watson Machine Learning service measures token usage in Resource Units (RU), and the equivalent of 1 RU = 1000 tokens. In addition, each model has its own price per RU.</p> <p></p> <p>When performing prompt engineering, we have used the large llama model, which implies a higher economic cost per RU. Normally, for complex tasks that need to be solved with prompt engineering, we will rely on the large models. On the other hand, applying prompt tuning on the small version of the models is enough to get the desired outputs. </p> <ul> <li>Capacity Unit Hours (CUH). Measures all Watson Machine Learning activity except for foundation model inferencing. The tuning experiment increases the CUH consumption. Depending on the Watson Machine Learning paid plan purchased, we have a different economic cost.</li> </ul> <p></p> <p>In the Essentials plan we would pay according to the CUH metric. On the other hand, in the Standard plan we would simply have a usage limit per month. In this lab, we have only run the experiment once, but it is likely that it should be run some more times to adjust the parameters and improve the loss function. Prompt tuning can be understood as an early investment, with which we will achieve long-term savings due to token inference costs.</p> <p>Consult this link for further information about Watson Machine Learning plans and compute usage.</p> <ul> <li> <p>Less interpretability with prompt tuning, since soft prompts are unrecognizable to the human eye. In contrast, with prompt engineering the interpretability is higher, since the prompts are manually crafted.</p> </li> <li> <p>Readily available data to perform prompt tuning for a specific task. For this lab, where prompt engineering returns satisfactory results, this point could be key. In the case where data is not readily available for the training set, it would be time consuming to create one. In addition, guidelines should be followed to keep control of the quality of the data.</p> </li> <li> <p>Output reliability. In this lab, we have not focused on getting a an evaluation metric, but rather on verifying that the models achieves outputs according to the business logic of the problem. If we wanted to further evaluate the outputs, we could run an inference of a test set, which is composed of a number of examples that are not included in the training set. In the following section, we will see this process implemented with code using a notebook.</p> </li> </ul>"},{"location":"lab-tuning-generative-ai/#prompt-tuning-with-a-notebook","title":"Prompt tuning with a Notebook","text":"<p>For this section of the lab, the experiment is not performed in the watsonx interface, instead, it is implemented in code using a notebook. Moreover, the use case is different and is based on an extracted dataset from HuggingFace.</p> <p>The notebook is prepared with the necessary code to run the data reading and preparation, the prompt tuning experiment, the test set inference and the evaluation of the tuned model and two other reference models. </p> <ol> <li> <p>Now, open and examine the notebook llama-2-13b NeuroPatents Tuning and Inference.ipynb by clicking the follosing link.</p> <p>This notebook shows how you could follow all the steps in a programatic way. Take your time to review it.</p> <ul> <li>llama-2-13b NeuroPatents Tuning and Inference.ipynb</li> </ul> </li> </ol> <p>Note</p> <p>If you want to edit the previous notebook in your own environment/workstation, download it using THIS LINK</p> <p>The End</p> <p>You have reached the end of this lab.</p>"},{"location":"prompt-engineering-exercise-answers/","title":"Lab 1 Answers","text":""},{"location":"prompt-engineering-exercise-answers/#prompt-engineering-exercise-answers","title":"Prompt engineering exercise answers","text":"Exercise Description 0. Zero-shot generation Generate a marketing message with 5 sentences 1. Generate sentences Write three sentences about birds 2. Working with JSON Generate a JSON file 3. Summarize Summarize a short story 4. Work with code Translate from C++ to C"},{"location":"prompt-engineering-exercise-answers/#0-zero-shot-generation","title":"0. Zero-shot generation","text":"<p>Goal</p> <p>Generate a marketing message with 5 sentences</p> <p>A possible solution</p> <p></p> <p>Info</p> <p>Los distintos modelos pueden requerir instrucciones diferentes. En el caso de los modelos m\u00e1s grandes y \"creativos\", puede ser necesario que las instrucciones sean m\u00e1s expl\u00edcitas. </p> <p>Es tentador pensar que los modelos m\u00e1s grandes son \"m\u00e1s inteligentes\". Hay algo de verdad en ello, pero estos modelos tambi\u00e9n pueden ser demasiado creativos. Como ha visto, puede que tenga que ser muy expl\u00edcito para obtener el resultado que busca. Tenga en cuenta que la creatividad no es necesariamente un problema. Este es uno de los atractivos de la IA generativa y de los modelos fundacionales. Pero si no est\u00e1 intentando generar contenidos creativos (como en este ejemplo: quiere que su correo electr\u00f3nico mencione los puntos espec\u00edficos y no a\u00f1ada contenidos ficticios), quiz\u00e1 deba considerar un modelo m\u00e1s peque\u00f1o (y menos creativo) o proporcionar indicaciones m\u00e1s estrictas para controlar los modelos.</p> <p>Tip</p> <p>Tambi\u00e9n puedes ver el efecto de cambiar algunos par\u00e1metros de configuraci\u00f3n:</p> <ul> <li>Temperatura: Cuanto mayor sea el valor, m\u00e1s creativo ser\u00e1 el modelo.</li> <li>Top P: Un valor m\u00e1s bajo significa menos variabilidad</li> <li>Top K: Un valor m\u00e1s bajo significa menos variabilidad</li> </ul> <p>Click here to go back to this exercise</p>"},{"location":"prompt-engineering-exercise-answers/#1-generate","title":"1. Generate","text":"<p>Goal</p> <p>Write three sentences about birds</p> <p>A possible solution</p> <p></p> <p>Info</p> <p>Los modelos manejan los requisitos de salida de forma diferente. Algunos modelos (como los modelos de flan) son mejores con la salida de texto (al menos con la indicaci\u00f3n zero-shot). Puede guiar la salida si actualiza el aviso con la sugerencia de una \"lista\". Otros modelos, como el mpt- 7b-instruct2, y en menor grado el modelo gpt-geox-20b, son m\u00e1s capaces de entender el concepto de salida de lista sin instrucciones adicionales.</p> <p>Click here to go back to this exercise</p>"},{"location":"prompt-engineering-exercise-answers/#2-working-with-json","title":"2. Working with JSON","text":"<p>Goal</p> <p>Generate a JSON file</p> <p>A possible solution</p> <p></p> <p>Info</p> <p>Mientras que los modelos flan han demostrado una buena capacidad para generar texto en lenguaje natural, no parecen entender c\u00f3mo generar una salida en formato JSON, al menos no con zero-shot. Los modelos mpt-7b-instruct2 y starcoder-15.5bmodel pueden generar una buena salida JSON. Diferentes modelos son entrenados/ajustados para diferentes tareas y habilidades. Deber\u00edas buscar modelos que est\u00e9n entrenados con datos espec\u00edficos del dominio con un enfoque en varias estructuras de datos y capacidades. Experimente tambi\u00e9n el uso de las Stop sequences para evitar que un modelo genere texto no deseado. </p> <p>Click here to go back to this exercise</p>"},{"location":"prompt-engineering-exercise-answers/#3-summarize","title":"3. Summarize","text":"<p>Goal</p> <p>Summarize one of the following short stories</p> <p>A possible solution</p> <p></p> <p>Click here to go back to this exercise</p>"},{"location":"prompt-engineering-exercise-answers/#4-code","title":"4. Code","text":"<p>Goal</p> <p>Translate from C++ to C</p> <p>A possible solution</p> <p></p> <p>Info</p> <p>Las tareas de codificaci\u00f3n requieren modelos b\u00e1sicos que comprendan la compleja estructura de salida del c\u00f3digo. Los modelos de lenguaje natural como la familia de modelos flan, no funcionan bien para los casos de uso de generaci\u00f3n y traducci\u00f3n de c\u00f3digo. El mejor modelo para estas tareas dentro de la biblioteca de IBM es el modelo starcoder-15.5b. Funciona bien en ambos tipos de tareas (generaci\u00f3n de c\u00f3digo y traducci\u00f3n de c\u00f3digo). Sin embargo, para la generaci\u00f3n de c\u00f3digo, es necesario realizar one-shot para guiar al modelo hacia la salida deseada.</p> <p>Click here to go back to this exercise</p>"},{"location":"prompt-engineering-exercises/","title":"Lab 1. Prompt Engineering","text":""},{"location":"prompt-engineering-exercises/#prompt-engineering-exercises","title":"Prompt engineering exercises","text":""},{"location":"prompt-engineering-exercises/#connect-to-the-environment","title":"Connect to the environment","text":"<ol> <li> <p>Click on this link to access the watsonx platform.</p> </li> <li> <p>Onces the watsonx console is opened, make sure to select the IBM Cloud account where you were invited, and the correct location (Dallas).</p> <p></p> </li> </ol>"},{"location":"prompt-engineering-exercises/#create-a-watsonxai-project","title":"Create a watsonx.ai project","text":"<ol> <li> <p>From the watsonx.ai console (Home window), click + to the right of Projects to create a new project.</p> <p></p> </li> <li> <p>Name the new project as &lt;Your Initials&gt; Prompt Workshop (where &lt;Your Initials&gt; is your name initials [i.e. John Smith would be JS]). Make sure the storage service is automatically selected. Then, click Create.</p> <p></p> </li> <li> <p>Next, you will link the Watson Machine Learning service to this watsonx project. To do this, move to the Manage window of the project. Select Services &amp; Integrations and click Associate service +.</p> <p></p> </li> <li> <p>On the next screen, select the Watson Machine Learning instance that is provided in this account.</p> <p></p> </li> </ol>"},{"location":"prompt-engineering-exercises/#create-a-new-prompt-session","title":"Create a new Prompt session","text":"<ol> <li> <p>From the Assets tab, click New asset +.</p> <p></p> </li> <li> <p>Select the Prompt lab option.</p> <p></p> </li> <li> <p>By default, the chat interface is shown. In this case, we want to do prompt engineering, so go to the Structured tab.</p> <p></p> </li> <li> <p>Take a moment to get familiar with the User Interface. Some of the key options are shown in the image below.</p> <p></p> </li> </ol>"},{"location":"prompt-engineering-exercises/#prompt-challenge-overview","title":"Prompt Challenge overview","text":"<p>You have to create prompts to cover the following use cases using watsonx.ai.</p> <p>Remember to test different models for each exercise to find which works best in each case.</p> <p>Tip</p> <p>Sample solutions can be found in the section Lab 1 Answers</p> <p>Exercises</p> Exercise Description 0. Zero-shot generation Generate a marketing message with 5 sentences 1. Generate sentences Write three sentences about birds 2. Working with JSON Generate a JSON file 3. Summarize Summarize a short story 4. Work with code Translate from C++ to C"},{"location":"prompt-engineering-exercises/#0-zero-shot-generation","title":"0. Zero-shot generation","text":"<p>Goal</p> <p>Generate a marketing message with 5 sentences.</p> <p>Instruction</p> <p>Genere un mensaje de marketing de 5 frases para una empresa con las caracter\u00edsticas dadas.</p> <p>Details that should be included</p> <ul> <li>Empresa: Golden Bank</li> <li>La oferta incluye: sin comisiones, tipo de inter\u00e9s del 2%, sin saldo m\u00ednimo </li> <li>Tono: Informativo</li> <li>Respuesta solicitada: Haga clic en el enlace </li> <li>Fecha de finalizaci\u00f3n: 15 de julio</li> </ul> <p>See Sample answer for a possible solution.</p>"},{"location":"prompt-engineering-exercises/#1-generate","title":"1. Generate","text":"<p>Goal</p> <p>Write three sentences about birds</p> <p>Example 1</p> <p>3 frases sobre cachorros:</p> <ul> <li>El cachorro giraba en c\u00edrculos intentando atrapar su cola, pero acababa   dando tumbos una y otra vez.</li> <li>Sus due\u00f1os se rieron a carcajadas, e incluso los dem\u00e1s cachorros del parque   se pararon a contemplar la tonter\u00eda.</li> <li>En cuanto los dos cachorros se encontraron en el parque, empezaron a mover    la cola y a dar saltitos de alegr\u00eda.</li> </ul> <p>Example 2</p> <p>3 frases sobre gatitos:</p> <ul> <li>La peque\u00f1a gatita sorbi\u00f3 la leche con su peque\u00f1a lengua rosada, haciendo un   simp\u00e1tico sonido de sorbo.</li> <li>La gatita mordisque\u00f3 la golosina, saboreando cada bocado de su delicioso  sabor.</li> <li>Nada m\u00e1s abrir el paquete, los ojos de la gatita se iluminaron de emoci\u00f3n.</li> </ul> <p>See Sample answer for a possible solution.</p>"},{"location":"prompt-engineering-exercises/#2-working-with-json","title":"2. Working with JSON","text":"<p>Goal</p> <p>Generate a JSON file</p> <p>Details to be included</p> <ul> <li>Nombre: Joe </li> <li>Edad: 25</li> <li>Tel\u00e9fono: 416-1234-567</li> <li>Tel\u00e9fono: 547-4034-240</li> <li>Direcci\u00f3n:<ul> <li>Ciudad: Markham</li> <li>Calle: Warden Avenue</li> <li>C\u00f3digo Postal: L6G 1C7</li> </ul> </li> </ul> <p>See Sample answer for a possible solution.</p>"},{"location":"prompt-engineering-exercises/#3-summarize","title":"3. Summarize","text":"<p>Goal</p> <p>Summarize one of these short stories</p> <p>Short story 1</p> <p>Un pajarillo piaba mientras recog\u00eda ramitas y trocitos de musgo en su pico, revoloteando de un lado a otro entre los \u00e1rboles. entre los \u00e1rboles. Con cada viaje, su nido tomaba forma, haci\u00e9ndose m\u00e1s acogedor y tentador. Y muy pronto hab\u00eda creado un hogar acogedor para criar a sus polluelos.</p> <p>Short story 2</p> <p>Nada m\u00e1s abrir el paquete, los ojos de la gatita se iluminaron de emoci\u00f3n. Se abalanz\u00f3 se abalanz\u00f3 sobre el nuevo juguete y lo pase\u00f3 por la habitaci\u00f3n con alegr\u00eda. Con un ronroneo de satisfacci\u00f3n se acurruc\u00f3 con su juguete, agradecida por el amor y la atenci\u00f3n de su cari\u00f1osa due\u00f1a.</p> <p>Short story 3</p> <p>El barco se agitaba en el mar embravecido por la tormenta. Olas tan altas como monta\u00f1as chocaban contra el casco, amenazando con hacer zozobrar el nav\u00edo. </p> <p>Pero el capit\u00e1n y la tripulaci\u00f3n se mantuvieron firmes, surcando las traicioneras aguas con destreza y determinaci\u00f3n, hasta que finalmente la tormenta amain\u00f3 y el barco emergi\u00f3 triunfante. tormenta amain\u00f3 y el barco emergi\u00f3 triunfante, maltrecho pero intacto.</p> <p>Short story 4</p> <p>En cuanto los dos perros se encontraron en el parque, sus colas empezaron a moverse y a saltar el uno alrededor del otro con alegr\u00eda. Sus due\u00f1os entablaron conversaci\u00f3n y pronto se dieron cuenta de que ten\u00edan mucho en com\u00fan, pues compart\u00edan su amor por los perros y el aire libre. </p> <p>Al final del d\u00eda se hab\u00edan forjado nuevas amistades, y tanto los perros como sus due\u00f1os salieron del parque con el coraz\u00f3n contento y la cola meneando.</p> <p>See Sample answer for a possible solution.</p>"},{"location":"prompt-engineering-exercises/#4-code","title":"4. Code","text":"<p>Goal</p> <p>Translate code from C++ to C</p> <p>Tip</p> <p>Use the copy button in the upper right corner of the code box to copy all the code at once.</p> <p>Code in C++ </p><pre><code>// Your C++ Hello World\n#include &lt;iostream&gt;\n\nint main() {\n    std::cout &lt;&lt; \"Hello World!\"; \n    return 0;\n}\n</code></pre> <p>See Sample answer for a possible solution.</p> <p>The End</p> <p>You have reached the end of this lab.</p>"}]}